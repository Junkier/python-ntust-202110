{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d04647",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 載入套件\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 取得 首頁 source code\n",
    "url = \"https://www.nownews.com/cat/column/\"\n",
    "\n",
    "headers= {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "}\n",
    "\n",
    "res = requests.get(url,headers=headers)\n",
    "soup = bs(res.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 抓取 1 ~ 10 篇文章連結\n",
    "links = []\n",
    "\n",
    "# 抓取 1~5 篇\n",
    "for ele in soup.select(\"div.sliderBlk a\"):\n",
    "    url = ele[\"href\"]\n",
    "    links.append(url)\n",
    "    \n",
    "# 抓取 6~10 篇\n",
    "for ele in soup.select(\"div.leftCol div.listBlk a\"):\n",
    "    links.append(ele[\"href\"])\n",
    "    \n",
    "for url in links:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56783f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 抓取本文資料 -> 收集成一個 function \n",
    "def parsing_content_data(content_soup):\n",
    "    \n",
    "    # 標題\n",
    "    title = content_soup.select(\"div.titleBlk h1\")[0].text\n",
    "\n",
    "    # 分類\n",
    "    # \\n -> 換行符號\n",
    "    category = content_soup.select(\"div.breadCrumbBlk\")[0].text\n",
    "    category = category.strip().replace(\"\\n\\n\",\" > \")\n",
    "\n",
    "    # 時間\n",
    "    # time = content_soup.select(\"div.titleBlk p.time\")[0].text.strip()\n",
    "    # 時間格式更新\n",
    "    raw_time = content_soup.select(\"div.titleBlk p.time\")[0].text.strip()\n",
    "    time = raw_time.split(\"｜\")[0]\n",
    "\n",
    "    # 內容\n",
    "    article = content_soup.select(\"div.leftCol article\")[0]\n",
    "\n",
    "    # 拔除 div.ad-blk1 , ul.related 標籤\n",
    "    if len(article.select(\"div.ad-blk1\"))>0:\n",
    "        article.select(\"div.ad-blk1\")[0].extract()\n",
    "    if len(article.select(\"ul.related\")) >0:\n",
    "        article.select(\"ul.related\")[0].extract()\n",
    "\n",
    "    content = article.text.strip()\n",
    "    \n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"category\" : category,\n",
    "        \"time\"  : time,\n",
    "        \"content\" : content\n",
    "    }\n",
    "\n",
    "\n",
    "def get_source_code(url):\n",
    "    headers= {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "    }\n",
    "    res = requests.get(url,headers=headers)\n",
    "    soup = bs(res.text,\"lxml\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "url3 = links[8]\n",
    "print(url3)\n",
    "soup3 = get_source_code(url3)\n",
    "parsing_content_data(soup3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f22fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 抓取 1 ~ 10 篇文章本文\n",
    "dataList = []\n",
    "for url in links:\n",
    "    content_soup = get_source_code(url)\n",
    "    data = parsing_content_data(content_soup)\n",
    "    dataList.append(data)\n",
    "    \n",
    "    print(\"{} is ok.\".format(url))\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 資料輸出成檔案 (資料落地)\n",
    "for data in dataList:\n",
    "\n",
    "    file_name = \"sample/{}.txt\".format(data[\"title\"])\n",
    "    with open(file_name,\"w\",encoding=\"utf8\") as out_file:\n",
    "        \n",
    "        record = \"\"\n",
    "        \n",
    "        # 資料全部讀取完 , 再一次寫出檔案\n",
    "        for key in data:\n",
    "            record += \"{}:{}\\n\".format(key , data[key])\n",
    "            \n",
    "        out_file.write(record)\n",
    "\n",
    "        # 每讀一段資料 , 就寫出檔案一次 (不佳)\n",
    "        # for key in data:\n",
    "        #     msg = \"{}:{}\\n\".format(key , data[key])\n",
    "        #     out_file.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 透過 API 抓 JSON 資料\n",
    "\n",
    "# 1. 抓取 txtPageNo , 取得 1st 參數\n",
    "pid = soup.select(\"input#txtPageNo\")[0][\"value\"]\n",
    "print(pid)\n",
    "\n",
    "# 2. 向 API 發 requests , 取得文章列表資料\n",
    "api = \"https://www.nownews.com/nn-client/api/v1/cat/column/?pid={}\".format(pid)\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "}\n",
    "\n",
    "res = requests.get(api,headers=headers)\n",
    "\n",
    "# response 用文字格式輸出\n",
    "# print(res.text)\n",
    "\n",
    "# response 用 json 格式輸出\n",
    "data = res.json()\n",
    "\n",
    "links2 = []\n",
    "\n",
    "for ele in data[\"data\"][\"newsList\"]:\n",
    "    print(ele[\"postTitle\"])\n",
    "    # print(ele[\"postUrl\"])\n",
    "    \n",
    "    url = \"https://www.nownews.com\" + ele[\"postUrl\"]\n",
    "#     print(url)\n",
    "    links2.append(url)\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "# ele 停留在最後一筆資料\n",
    "pid = ele[\"id\"]\n",
    "\n",
    "\n",
    "# 3. 再透過 API 的資料 , 取得下一次的 requests 參數\n",
    "for i in range(1,4):\n",
    "    api = \"https://www.nownews.com/nn-client/api/v1/cat/column/?pid={}\".format(pid)\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    res = requests.get(api,headers=headers)\n",
    "    \n",
    "    data = res.json()\n",
    "    \n",
    "    for ele in data[\"data\"][\"newsList\"]:\n",
    "        url = \"https://www.nownews.com\" + ele[\"postUrl\"]\n",
    "        links2.append(url)\n",
    "        \n",
    "    pid = ele[\"id\"]\n",
    "    \n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
